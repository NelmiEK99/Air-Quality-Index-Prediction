# -*- coding: utf-8 -*-
"""Air-Quality-Index-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eha0eWsV_0soJgw9Q6au2ql8O5O6WhqB
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, isnull
from pyspark.mllib.stat import Statistics
from pyspark.sql import Row
from pyspark.sql.types import TimestampType
from pyspark.sql.window import Window
from pyspark.sql import functions as F
from pyspark.sql.functions import col, mean, last, first
from pyspark.sql.functions import to_date
from datetime import datetime, timedelta
from pyspark.ml.feature import Imputer
from pyspark.sql.types import StructType, StructField, TimestampType
from pyspark.sql.types import DateType, StringType
from pyspark.sql.types import IntegerType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col
from pyspark.sql.functions import to_timestamp
import pyspark.sql.functions as sqlf
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

spark = SparkSession.builder \
    .appName("Air Quality Index Prediction") \
    .config("spark.sql.legacy.timeParserPolicy", "LEGACY") \
    .getOrCreate()

df = spark.read.csv('/content/sample_data/city_hour.csv', header=True, inferSchema=True)
df = df.withColumn("Datetime", to_timestamp(col("Datetime"), 'MM/dd/yyyy HH:mm'))
df.show()

rdd = df.rdd
print(rdd.take(5))

num_rows = rdd.count()
schema = df.schema

print("Shape of RDD: ({}, {})".format(num_rows, len(schema)))

for field in schema:
    print("Column: {}, Data Type: {}".format(field.name, field.dataType))

"""**Interpretation**


*   From the above information, dataset have 1 Categorical variables and 10 Numerical variables.
*  The data have total 11 variables and 707875 records.


"""

def compute_summary_statistics(rdd):
    # Calculate count, mean, sum, minimum, maximum, and standard deviation for each column
    summary_stats = rdd.map(lambda row: [(col, float(cell)) for col, cell in row.asDict().items() if isinstance(cell, (int, float))]) \
                       .filter(lambda x: len(x) > 0) \
                       .cache()

    column_count = summary_stats.count()
    summary_stats = summary_stats.flatMap(lambda x: x) \
                                 .groupByKey() \
                                 .map(lambda x: (x[0], (len(x[1]), sum(x[1]), sum(x[1])/len(x[1]), min(x[1]), max(x[1]), np.std(list(x[1])), np.percentile(list(x[1]), 25), np.percentile(list(x[1]), 75)))) \
                                 .collect()

    # Print summary statistics for each column
    for column, (count, total, mean, minimum, maximum, std_dev, q1, q3) in summary_stats:
        print("Summary statistics for column '{}':".format(column))
        print("  Count: {}".format(count))
        print("  Total: {}".format(total))
        print("  Mean: {}".format(mean))
        print("  Minimum: {}".format(minimum))
        print("  Maximum: {}".format(maximum))
        print("  Standard Deviation: {}".format(std_dev))
        print("  1st Quartile (Q1): {}".format(q1))
        print("  3rd Quartile (Q3): {}".format(q3))
        print()

# Call the function to compute summary statistics
compute_summary_statistics(rdd)

"""**Interpretation**

*   Summary statistics give maximum, minimum, average, first and third qurtile and standared deviation of the data.
*   Most of the variables having minimum value as 0.

#Data preparation and cleaning
"""

# Calculate missing values count for each column
missing_counts = rdd.map(lambda row: [(col, 1) if cell is None else (col, 0) for col, cell in row.asDict().items()]) \
                    .flatMap(lambda x: x) \
                    .reduceByKey(lambda a, b: a + b)

# Calculate percentage of missing values for each column
total_rows = rdd.count()
missing_percentage = missing_counts.map(lambda x: (x[0], (x[1], round((x[1] / total_rows) * 100, 2))))

# Filter columns with missing values
missing_columns = missing_percentage.filter(lambda x: x[1][0] != 0) \
                                    .map(lambda x: (x[0], x[1][0], x[1][1]))

# Convert RDD to DataFrame for easier manipulation
columns = ["Column Name", "No of missing values", "Percent of missing values"]
missing_df = spark.createDataFrame(missing_columns, columns)

# Sort DataFrame by percentage of missing values in descending order
missing_df = missing_df.orderBy("Percent of missing values", ascending=False)

# Display DataFrame
missing_df.show()

# Function to create heatmap of missing values
def plot_missing_values_heatmap(rdd):
    # Collect RDD to the driver node
    data = rdd.collect()

    # Extract column names
    columns = rdd.flatMap(lambda row: row.asDict().keys()).distinct().collect()

    # Initialize a 2D list to store missing values
    missing_values_matrix = []

    # Iterate over each row to create the missing values matrix
    for row in data:
        missing_values_matrix.append([1 if cell is None else 0 for cell in row])

    # Plot the heatmap
    plt.figure(figsize=(10, 6))
    plt.imshow(missing_values_matrix, cmap='OrRd', aspect='auto')
    plt.xticks(range(len(columns)), columns, rotation=90)
    plt.xlabel('Columns')
    plt.ylabel('Rows')
    plt.title('Heatmap of Missing Values')
    plt.colorbar().set_label('Missing Values')
    plt.show()

# Call the function to plot the heatmap
plot_missing_values_heatmap(rdd)

"""**Interpretation**



*   From the above heatmep, we can say that many missing values are present in the dataset.
*  We cannot remove the missing rows from the dataset as we will miss on timestamps.

**Interpretations**

*  We cannot impute the missing values with group by datetime and its average, because dataset has missing values for continuous 3-4 days.
*   We will first convert Datetime variable into datetime data type.
"""

df.dtypes

from pyspark.sql.functions import to_timestamp, col

df = df.withColumn("Datetime", to_timestamp(col("Datetime"), 'MM/dd/yyyy HH:mm'))
df.show()

"""
*   Datetime variable is changed into datetime datatype.


"""

df = df.withColumnRenamed("PM2.5", "PM25")
# Show the resulting DataFrame
df.show()

def drop_null_columns(df):
    """
    This function drops columns containing all null values.
    :param df: A PySpark DataFrame
    """
    # Calculate the count of null values for each column
    null_counts = df.agg(*[sqlf.sum(sqlf.when(sqlf.col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns])
    # Collect the counts into a dictionary
    null_counts_dict = null_counts.collect()[0].asDict()
    # Identify columns with all null values
    to_drop = [col for col, count in null_counts_dict.items() if count == df.count()]
    # Drop identified columns
    df = df.drop(*to_drop)

    return df

# Example usage
df = drop_null_columns(df)
df.show()

rdd = df.rdd
print(rdd.take(5))

"""**Interpretations**

* Still missing values are present in the data, now we can not fill it by average of year, if we fill the value by average of year the data integrity will be changed, so we concidered that value of the pollutants are same throughout the month. So we can use the backward or forward fill technique for imputing the missing values.




"""

df["DateTime"].dtype

def backward_fill(df):
    # Get the column names and their data types
    schema = df.schema

    # Iterate over each column and backward fill missing values, skipping date and string types
    for field in schema.fields:
        column = field.name
        data_type = field.dataType

        # Skip date and string columns
        if isinstance(data_type, TimestampType) or isinstance(data_type, StringType):
            continue

        last_valid_value = df.select(last(col(column), ignorenulls=True)).collect()[0][0]
        df = df.fillna(last_valid_value, subset=[column])

    return df

# Backward fill missing values
df_filled = backward_fill(df)
df_filled.show()

def forward_fill(df):
    # Get the column names and their data types
    schema = df.schema

    # Iterate over each column and forward fill missing values, skipping date and string types
    for field in schema.fields:
        column = field.name
        data_type = field.dataType

        # Skip date and string columns
        if isinstance(data_type, TimestampType) or isinstance(data_type, StringType):
            continue

        first_valid_value = df.select(first(col(column), ignorenulls=True)).collect()[0][0]
        df = df.fillna(first_valid_value, subset=[column])

    return df

# Forward fill missing values
df_filled = forward_fill(df_filled)
df_filled.show()

rdd_filled = df_filled.rdd
print(rdd_filled.take(5))

# Function to create heatmap of missing values
def plot_missing_values_heatmap(rdd_filled):
    # Collect RDD to the driver node
    data = rdd_filled.collect()

    # Extract column names
    columns = rdd_filled.flatMap(lambda row: row.asDict().keys()).distinct().collect()

    # Initialize a 2D list to store missing values
    missing_values_matrix = []

    # Iterate over each row to create the missing values matrix
    for row in data:
        missing_values_matrix.append([1 if cell is None else 0 for cell in row])

    # Plot the heatmap
    plt.figure(figsize=(10, 6))
    plt.imshow(missing_values_matrix, cmap='OrRd', aspect='auto')
    plt.xticks(range(len(columns)), columns, rotation=90)
    plt.xlabel('Columns')
    plt.ylabel('Rows')
    plt.title('Heatmap of Missing Values')
    plt.colorbar().set_label('Missing Values')
    plt.show()

# Call the function to plot the heatmap
plot_missing_values_heatmap(rdd_filled)

# Convert RDD to DataFrame
df = spark.createDataFrame(rdd_filled)
# Extract column names
columns = df.columns

# Convert each column in the DataFrame to an RDD of values
rdd_columns = [df.select(col).rdd.flatMap(lambda x: x).collect() for col in columns]

# Plot boxplots for each column
plt.figure(figsize=(12, 8))
sns.boxplot(data=rdd_columns)
plt.title("Boxplot of Data Columns")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Interpretation**

* From the box plot, can say that the outliers are present in the data, but this is the time series data
* So we can not remove outliers, if we do this the continuity will not remain for the data.

# **Visualization**

## Standadize data
"""

df_filled.createOrReplaceTempView("pollution_data")

# Calculate mean and standard deviation for each pollutant
pollutants = ['PM25', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3']
stats = spark.sql("""
SELECT
    AVG(PM25) as mean_PM25, STDDEV(PM25) as stddev_PM25,
    AVG(PM10) as mean_PM10, STDDEV(PM10) as stddev_PM10,
    AVG(NO) as mean_NO, STDDEV(NO) as stddev_NO,
    AVG(NO2) as mean_NO2, STDDEV(NO2) as stddev_NO2,
    AVG(NOX) as mean_NOx, STDDEV(NOX) as stddev_NOx,
    AVG(NH3) as mean_NH3, STDDEV(NH3) as stddev_NH3,
    AVG(CO) as mean_CO, STDDEV(CO) as stddev_CO,
    AVG(SO2) as mean_SO2, STDDEV(SO2) as stddev_SO2,
    AVG(O3) as mean_O3, STDDEV(O3) as stddev_O3
FROM pollution_data
""").collect()[0]

# Create a new DataFrame with standardized values
standardized_df = spark.sql(f"""
SELECT
    City, Datetime,
    ((PM25 - {stats['mean_PM25']}) / {stats['stddev_PM25']}) as PM25,
    ((PM10 - {stats['mean_PM10']}) / {stats['stddev_PM10']}) as PM10,
    ((NO - {stats['mean_NO']}) / {stats['stddev_NO']}) as NO,
    ((NO2 - {stats['mean_NO2']}) / {stats['stddev_NO2']}) as NO2,
    ((NOX - {stats['mean_NOx']}) / {stats['stddev_NOx']}) as NOx,
    ((NH3 - {stats['mean_NH3']}) / {stats['stddev_NH3']}) as NH3,
    ((CO - {stats['mean_CO']}) / {stats['stddev_CO']}) as CO,
    ((SO2 - {stats['mean_SO2']}) / {stats['stddev_SO2']}) as SO2,
    ((O3 - {stats['mean_O3']}) / {stats['stddev_O3']}) as O3
FROM pollution_data
""")

standardized_df.show()

standardized_df = standardized_df.orderBy("Datetime")

standardized_df = standardized_df.withColumn('Datetime', F.to_timestamp('Datetime'))

data = standardized_df.select('Datetime', 'PM25','PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3').collect()

dates = [row['Datetime'] for row in data]
pollutants = {'PM25': [], 'PM10': [], 'NO': [], 'NO2': [], 'NOx': [], 'NH3': [], 'CO': [], 'SO2': [], 'O3': []}

for row in data:
    for pollutant in pollutants.keys():
        pollutants[pollutant].append(row[pollutant])

# Plotting
plt.figure(figsize=(15, 10))
for pollutant, values in pollutants.items():
    plt.plot(dates, values, label=pollutant)

plt.title('Time Series of All Pollutants')
plt.xlabel('Date')
plt.ylabel('Concentration')
plt.legend(title='Pollutant', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.tight_layout()
plt.show()

data = standardized_df.select('Datetime', 'PM25', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3').collect()

dates = [row['Datetime'] for row in data]
pollutants = {
    'PM2.5': [row['PM25'] for row in data],
    'PM10': [row['PM10'] for row in data],
    'NO': [row['NO'] for row in data],
    'NO2': [row['NO2'] for row in data],
    'NOx': [row['NOx'] for row in data],
    'NH3': [row['NH3'] for row in data],
    'CO': [row['CO'] for row in data],
    'SO2': [row['SO2'] for row in data],
    'O3': [row['O3'] for row in data]
}

colors = {
    'PM2.5': 'blue',
    'PM10': 'green',
    'NO': 'red',
    'NO2': 'cyan',
    'NOx': 'magenta',
    'NH3': 'yellow',
    'CO': 'black',
    'SO2': 'orange',
    'O3': 'purple'
}

fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(15, 25))
axs = axs.flatten()

for i, (pollutant, values) in enumerate(pollutants.items()):
    axs[i].plot(dates, values, label=pollutant, color=colors[pollutant])
    axs[i].set_title(f'Time Series of {pollutant}')
    axs[i].set_xlabel('Date')
    axs[i].set_ylabel('Concentration')
    axs[i].legend()
    axs[i].grid(True)


plt.tight_layout()
plt.show()

standardized_df.createOrReplaceTempView("pollution_data")

# calculate total of each pollutant for each city
total_pollutants_sql = """
SELECT City,
       SUM(PM25) as Total_PM25,
       SUM(PM10) as Total_PM10,
       SUM(NO) as Total_NO,
       SUM(NO2) as Total_NO2,
       SUM(NOX) as Total_NOx,
       SUM(NH3) as Total_NH3,
       SUM(CO) as Total_CO,
       SUM(SO2) as Total_SO2,
       SUM(O3) as Total_O3
FROM pollution_data
GROUP BY City
"""

total_pollutants_df = spark.sql(total_pollutants_sql)

total_pollutants_df.show()

total_pollutants_df.createOrReplaceTempView("city_pollutant_totals")

import pandas as pd
plot_data = total_pollutants_df.collect()

pandas_df = pd.DataFrame(plot_data)
pandas_df.columns = ['City', 'Total_PM25', 'Total_PM10', 'Total_NO', 'Total_NO2', 'Total_NOx', 'Total_NH3', 'Total_CO', 'Total_SO2', 'Total_O3']

cities = pandas_df['City'].unique()
pollutants = ['Total_PM25', 'Total_PM10', 'Total_NO', 'Total_NO2', 'Total_NOx', 'Total_NH3', 'Total_CO', 'Total_SO2', 'Total_O3']

num_cities = len(cities)
fig, axes = plt.subplots(nrows=(num_cities // 2 + num_cities % 2), ncols=2, figsize=(12, 4 * (num_cities // 2 + num_cities % 2)))

axes = axes.flatten()

for index, city in enumerate(cities):
    city_data = pandas_df[pandas_df['City'] == city]
    city_data.set_index('City', inplace=True)
    city_data[pollutants].plot(kind='bar', ax=axes[index], title=f"Total Pollutant Concentrations in {city}")
    axes[index].set_ylabel('Total Concentration')
    axes[index].set_xlabel('Pollutants')
    axes[index].tick_params(axis='x', rotation=45)
    axes[index].legend(title='Pollutants')

plt.tight_layout()
plt.show()

total_pollutants_df = spark.sql("SELECT * FROM city_pollutant_totals")
plot_data = total_pollutants_df.collect()

pandas_df = pd.DataFrame(plot_data)
pandas_df.columns = ['City', 'Total_PM25', 'Total_PM10', 'Total_NO', 'Total_NO2', 'Total_NOx', 'Total_NH3', 'Total_CO', 'Total_SO2', 'Total_O3']

pandas_df.set_index('City', inplace=True)

# Plotting
fig, ax = plt.subplots(figsize=(14, 8))

pollutants = ['Total_PM25', 'Total_PM10', 'Total_NO', 'Total_NO2', 'Total_NOx', 'Total_NH3', 'Total_CO', 'Total_SO2', 'Total_O3']
x = np.arange(len(pandas_df.index))
width = 0.1

for i, pollutant in enumerate(pollutants):
    ax.bar(x + i * width, pandas_df[pollutant], width, label=pollutant)

ax.set_xlabel('Cities')
ax.set_ylabel('Total Concentrations')
ax.set_title('Total Pollutant Concentrations by City')
ax.set_xticks(x + width * (len(pollutants)-1) / 2)
ax.set_xticklabels(pandas_df.index)
ax.legend(title="Pollutants")

fig.tight_layout()
plt.xticks(rotation=45)
plt.show()

standardized_df.createOrReplaceTempView("standardized_data")

# Aggregate data by city
agg_df = spark.sql("""
    SELECT
        City,
        Datetime,
        AVG(PM25) as avg_PM25,
        AVG(PM10) as avg_PM10,
        AVG(NO) as avg_NO,
        AVG(NO2) as avg_NO2,
        AVG(NOx) as avg_NOX,
        AVG(NH3) as avg_NH3,
        AVG(CO) as avg_CO,
        AVG(SO2) as avg_SO2,
        AVG(O3) as avg_O3
    FROM standardized_data
    GROUP BY City, Datetime
""")

# Retrieve time series data for each city
cities = agg_df.select("City").distinct().rdd.flatMap(lambda x: x).collect()

# Plot time series data for each city
for city in cities:
    city_data = agg_df.filter(agg_df.City == city).toPandas()
    city_data.set_index('Datetime', inplace=True)

    plt.figure(figsize=(10, 6))
    for column in city_data.columns[1:]:
        plt.plot(city_data.index, city_data[column], label=column)

    plt.title(f'Time Series Data for {city}')
    plt.xlabel('Date')
    plt.ylabel('Average Standardized Value')
    plt.xticks(rotation=45)
    plt.legend()
    plt.show()

"""# **Calculate AQI and Predict AQI Values**

"""

# Function to calculate PM2.5 AQI
def calculate_pm25_aqi(concentration):
    if concentration <= 12.0:
        return round((50/12) * concentration)
    elif concentration <= 35.4:
        return round((49/23.4) * (concentration - 12.1) + 51)
    elif concentration <= 55.4:
        return round((49/20) * (concentration - 35.5) + 101)
    elif concentration <= 150.4:
        return round((99/94.9) * (concentration - 55.5) + 151)
    elif concentration <= 250.4:
        return round((99/99.9) * (concentration - 150.5) + 201)
    elif concentration <= 350.4:
        return round((99/99.9) * (concentration - 250.5) + 301)
    elif concentration <= 500.4:
        return round((99/149.9) * (concentration - 350.5) + 401)
    else:
        return 500  # Maximum AQI value

# Register the UDF
calculate_pm25_aqi_udf = F.udf(calculate_pm25_aqi, IntegerType())

# Function to calculate CO AQI
def calculate_co_aqi(concentration):
    concentration_ppb = concentration * 1000  # Convert from ppm to ppb
    if concentration_ppb <= 4500:
        return round((50/4500) * concentration_ppb)
    elif concentration_ppb <= 9500:
        return round((49/5000) * (concentration_ppb - 4501) + 51)
    elif concentration_ppb <= 12500:
        return round((49/3000) * (concentration_ppb - 9501) + 101)
    elif concentration_ppb <= 15500:
        return round((99/3000) * (concentration_ppb - 12501) + 151)
    elif concentration_ppb <= 30500:
        return round((99/15000) * (concentration_ppb - 15501) + 201)
    elif concentration_ppb <= 40500:
        return round((99/10000) * (concentration_ppb - 30501) + 301)
    elif concentration_ppb <= 50500:
        return round((99/10000) * (concentration_ppb - 40501) + 401)
    else:
        return 500  # Maximum AQI value

# Register the UDF
calculate_co_aqi_udf = F.udf(calculate_co_aqi, IntegerType())

# Function to calculate SO2 AQI
def calculate_so2_aqi(concentration):
    concentration_ppb = concentration
    if concentration_ppb <= 35:
        return round((50/35) * concentration_ppb)
    elif concentration_ppb <= 75:
        return round((49/40) * (concentration_ppb - 36) + 51)
    elif concentration_ppb <= 185:
        return round((49/110) * (concentration_ppb - 76) + 101)
    elif concentration_ppb <= 304:
        return round((99/119) * (concentration_ppb - 186) + 151)
    elif concentration_ppb <= 604:
        return round((99/300) * (concentration_ppb - 305) + 201)
    elif concentration_ppb <= 804:
        return round((99/200) * (concentration_ppb - 605) + 301)
    elif concentration_ppb <= 1004:
        return round((99/200) * (concentration_ppb - 805) + 401)
    else:
        return 500  # Maximum AQI value

# Register the UDF
calculate_so2_aqi_udf = F.udf(calculate_so2_aqi, IntegerType())

# Function to calculate O3 AQI for 8-hour average
def calculate_o3_aqi(concentration):
    if concentration <= 54:
        return round((50/54) * concentration)
    elif concentration <= 70:
        return round((49/16) * (concentration - 55) + 51)
    elif concentration <= 85:
        return round((49/15) * (concentration - 71) + 101)
    elif concentration <= 105:
        return round((99/20) * (concentration - 86) + 151)
    elif concentration <= 200:
        return round((99/95) * (concentration - 106) + 201)
    else:
        return 500  # Maximum AQI value for concentrations above 200

# Register the UDF
calculate_o3_aqi_udf = F.udf(calculate_o3_aqi, IntegerType())

# Function to calculate NOx AQI
def calculate_nox_aqi(concentration):
    if concentration <= 53:
        return round((50/53) * concentration)
    elif concentration <= 100:
        return round((49/47) * (concentration - 54) + 51)
    elif concentration <= 360:
        return round((49/260) * (concentration - 101) + 101)
    elif concentration <= 649:
        return round((99/289) * (concentration - 361) + 151)
    elif concentration <= 1249:
        return round((99/600) * (concentration - 650) + 201)
    elif concentration <= 1649:
        return round((99/400) * (concentration - 1250) + 301)
    elif concentration <= 2049:
        return round((99/400) * (concentration - 1650) + 401)
    else:
        return 500  # Maximum AQI value

# Register the UDF
calculate_nox_aqi_udf = F.udf(calculate_nox_aqi, IntegerType())

df.withColumn("NO2_AQI", calculate_nox_aqi_udf(df["NO2"])).show(n=700)

df.withColumn("NOX_AQI", calculate_nox_aqi_udf(df["NOX"])).show(n=700)

df.withColumn("O3_AQI", calculate_o3_aqi_udf(df["O3"])).show(n=700)

df.withColumn("SO2_AQI", calculate_so2_aqi_udf(df["SO2"])).show(n=700)

df.withColumn("CO_AQI", calculate_co_aqi_udf(df["CO"])).show(n=700)

df.show()

df = df.withColumn("AQI_PM25", calculate_pm25_aqi_udf(df["PM25"]))
df = df.withColumn("AQI_NO2", calculate_nox_aqi_udf(df["NO2"]))
df = df.withColumn("AQI_SO2", calculate_so2_aqi_udf(df["SO2"]))
df = df.withColumn("AQI_CO", calculate_co_aqi_udf(df["CO"]))
df = df.withColumn("AQI_O3", calculate_o3_aqi_udf(df["O3"]))

df = df.withColumn("AQI", max_aqi_udf(col("AQI_PM25"), col("AQI_NO2"), col("AQI_SO2"), col("AQI_CO"), col("AQI_O3")))

df = df.drop("AQI_PM25", "AQI_NO2", "AQI_SO2", "AQI_CO", "AQI_O3")
df.show()

def assign_aqi_bucket(df):
    return df.withColumn(
        "AQI_Bucket",
        when((col("AQI") >= 0) & (col("AQI") <= 50), "Good")
        .when((col("AQI") > 50) & (col("AQI") <= 100), "Satisfactory")
        .when((col("AQI") > 100) & (col("AQI") <= 200), "Moderately Polluted")
        .when((col("AQI") > 200) & (col("AQI") <= 300), "Poor")
        .when((col("AQI") > 300) & (col("AQI") <= 400), "Very Poor")
        .when((col("AQI") > 400) & (col("AQI") <= 500), "Severe")
    )

from pyspark.sql.functions import col

null_counts = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])
null_counts.show()

df_clean = df.dropna(how='all')
df_clean.show()

for column in df.columns[2:10]:
    print(f'The impact of {column} on AQI')

    sampled_df = df.select(column, "AQI").sample(fraction=0.1, withReplacement=False)

    pd_sampled = sampled_df.toPandas()

    sns.scatterplot(x=column, y='AQI', data=pd_sampled, marker="o", sizes=200, color="r", label=column)
    plt.legend()
    plt.show()

df_clean

"""### Time-based feature adding"""

from pyspark.sql.functions import year, month, dayofmonth, hour

df_clean = df_clean.withColumn("year", year(col("Datetime")))
df_clean = df_clean.withColumn("month", month(col("Datetime")))
df_clean = df_clean.withColumn("day", dayofmonth(col("Datetime")))
df_clean = df_clean.withColumn("hour", hour(col("Datetime")))

"""### Group dataset by each region"""

regions = df_clean.select("City").distinct().rdd.flatMap(lambda x: x).collect()

region_data_frames = {region: df_clean.filter(col("City") == region) for region in regions}

len(regions)

"""### Regression Model"""

from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor

assembler = VectorAssembler(
    inputCols=["year", "month", "day", "hour", "lag_1"],
    outputCol="features"
)

rf = RandomForestRegressor(featuresCol="features", labelCol="AQI", numTrees=5, maxDepth=5)

pipeline = Pipeline(stages=[assembler, rf])

"""### Function to plot predictions"""

import matplotlib.pyplot as plt
from pyspark.sql.functions import to_date

def aqi_prediction(lr_predictions):
  lr_predictions = lr_predictions.withColumn("Datetime", to_date(col("Datetime")))

  actual_aqi = lr_predictions.select("AQI").rdd.flatMap(lambda x: x).collect()
  predicted_aqi = lr_predictions.select("prediction").rdd.flatMap(lambda x: x).collect()
  dates = lr_predictions.select("Datetime").rdd.flatMap(lambda x: x).collect()

  import pandas as pd
  plot_df = pd.DataFrame({'Datetime': dates, 'Actual AQI': actual_aqi, 'Predicted AQI': predicted_aqi})
  plot_df.sort_values('Datetime', inplace=True)

  plt.figure(figsize=(10, 6))
  plt.plot(plot_df['Datetime'], plot_df['Actual AQI'], label="Actual AQI")
  plt.plot(plot_df['Datetime'], plot_df['Predicted AQI'], label="Predicted AQI")
  plt.xlabel("Datetime")
  plt.ylabel("AQI")
  plt.title("Actual vs Predicted AQI")
  plt.legend()
  plt.show()

"""### Predicting AQI for each region"""

from pyspark.sql.functions import lag, col
from pyspark.sql.window import Window

# Create a window specification
windowSpec = Window.partitionBy("City").orderBy("Datetime")

for region, frame in region_data_frames.items():

  # Create lag features; here we lag by one day (assuming data is hourly and each day has 24 records)
  frame = frame.withColumn("lag_1", lag("AQI", 24).over(windowSpec))
  # Drop rows with nulls that were created by lagging
  frame = frame.na.drop()

  df = frame.sort(col("Datetime"))
  total_rows = df.count()
  cutoff_row = int(total_rows * 0.8)
  cutoff_date = df.select("Datetime").collect()[cutoff_row][0]

  # Split the data based on the cutoff date
  train_data = frame.where(col("Datetime") < cutoff_date)
  test_data = frame.where(col("Datetime") >= cutoff_date)
  print(train_data)
  model = pipeline.fit(train_data)

  # Predict on the test data
  predictions = model.transform(test_data)
  predictions.select("City", "aqi", "prediction").show()
  aqi_prediction(predictions)

"""### Conclusion

These features are not sufficient to predict the AQI values correctly. We need to capture seasonal effects, weather patterns etc to correctly predict the AQI values.

"""